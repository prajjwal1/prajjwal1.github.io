---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        AI Resident<br>
        Facebook AI Research<br>
        prajj at fb dot com
    </div>
</div>
<hr>

<a name="/news"></a>

# News

* [Apr 22] I'll be joining Meta AI (formerly FAIR) as an AI Resident at Menlo Park.
* [Oct 21]  Survey paper has been accepted at AAAI 2022.
* [Sept 21] Paper accepted at EMNLP Insights from Negative Results Workshop 2021.
* [Oct 20] Volunteer for Neurips 2020
* [Oct 20] Declared winner of Pytorch Global Hackathon 2020 in the responsible AI category
* [Aug 20] I'll be joining UT Dallas as an MSCS student
* [June 20] Will serve as a volunteer for [ACL](https://acl2020.org/), Seattle 2020
* [June 20] Secured 18th spot at CVPR 2020 [VQA challenge](https://visualqa.org/roe)
* [June 20] Will serve as a volunteer for [ICML](https://icml.cc/Conferences/2020), Vienna 2020
* [May 20] My paper on Adaptive Transformers is available on [arxiv](https://arxiv.org/abs/2005.07486)
* [Apr 20] 1 Paper accepted at ACL SRW 2020
* [Apr 20] I'll be a volunteer for ICLR 2020
* [Feb 20] I joined Siemens again as a research intern to work on Predictive Maintenance.
* [Nov 19] I’ll be doing the Poster presentation at ICCV 2019, Seoul.
* [August 19] 1 paper accepted at ICCV workshop 2019
* [Jan 19] I joined Siemens as a research intern (Autonomous Navigation group)

<div id="read-more-button">
    <a nohref>Read more</a>
</div>

<hr>

<a name="/bio"></a>

# Bio

I am an AI Resident at [Meta AI Research](https://ai.facebook.com) (FAIR) in the [Conversational AI team](https://ai.facebook.com/research/conversational-ai/) working
under [Chinnadhurai Sankar](https://chinnadhurai.github.io).
Previously, I was a CS graduate student at the University of Texas Dallas where I worked on commonsense reasoning under
[Prof. Vincent Ng](http://www.hlt.utdallas.edu/~vince/). My thesis is about improving commonsense reasoning through adversarial
learning (coming soon). My research interests lie in the general area of deep learning and representation learning, especially at the intersection
of computer vision and language.

**Vision**
I have previously worked on numerous domains in computer vision during my internships which includes
[Person Re-Identification](https://prajjwal1.github.io/publications/IncrementalPersonReid),
[Depth Perception and 3D Object Detection](https://www.youtube.com/watch?v=vlDTgj3Kut8). Out of my interests,
I've worked extensively on visual recognition based tasks and
[generalization of detection models](https://prajjwal1.github.io/publications/GenDetectionIccvw19).
I've also written numerous [blog posts](https://prajjwal1.github.io/blog/) about it.

**Language**
To explore further interests, I've made a transition to NLP especially language understanding and how we can form
generalized representations which can be harnessed for subsequent downstream tasks. I care about the accessibility
and impact marked by my research. Specifically, I looked at
[Adaptive methods](https://prajjwal1.github.io/publications/adaptive_tfmr_acl_srw_2020) and how can they be used to
create efficient Multi-modal transformer-based architectures. Recently I won
[Pytorch Global Summer Hackathon](https://pytorch.org/blog/announcing-the-winners-of-the-2020-global-pytorch-summer-hackathon/)
for my project [Fluence](https://github.com/prajjwal1/fluence). It focuses on addressing the task of language
understanding responsibly (fairness and computational efficiency).

My recent projects have looked more closely at reasoning.
[Generalization in NLI ](https://arxiv.org/abs/2110.01518) project looked at how out-of-domain generalization in
pre-trained Language Models can be improved from biased data (MNLI -> HANS). My graduate research looked specifically
at commonsense reasoning within pre-trained Langauge Models. The [AAAI survey](https://arxiv.org/abs/2201.12438) provides
a comprehensive look at the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and
generation.


I created a [Youtube channel](https://youtube.com/c/aijournal) to disseminate technical content from research papers.
After graduate school, my interest has changed about creating content as a result of which this project is
unmaintained right now.

<a name="/publications"></a>

# Publications

<a name="/commonsense-survey"></a>
<h2 class="pubt">Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava and Vincent Ng</span><br>
    <span class="conf">AAAI 2022</span>
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2201.12438">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/aaai-22/survey_poster.pdf">Poster</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/commonsense_survey_aaai_22.bib">Bibtex</a>
    </span>
</p>
<img src="/img/survey/probe.jpg">
<hr>

<a name="/generalization-nli"></a>
<h2 class="pubt">Generalization in NLI: Ways to [Not] Go Beyond Simple Heuristics</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava, Aleksander Drozd, Anna Rogers</span><br>
    <span class="conf">EMNLP Workshop on Insights from Negative Results 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2110.01518">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/generalize_lm_nli">Code (Huggingface)</a>
        <a target="_blank" href="https://github.com/vecto-ai/langmo">Code (Pytorch Lightning)</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/generalize_lm_nli.bib">Bibtex</a>
        <a target="_blank" href="https://www.youtube.com/watch?v=ByQu3J6Ji7E">Presentation video</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/emnlp-21/NLI_Generalization_EMNLP_Poster.pdf">Poster</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/emnlp-21/chinese_room_presentation.pdf">Slides</a>

    </span>
</p>
<img src="/img/generalization_nli/MNLI_roberta-large.png">
<hr>

<a name="/adaptive"></a>
<h2 class="pubt">Adaptive Transformers for Learning Multimodal Representations</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava</span><br>
    <span class="conf">ICLR 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2005.07486">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/adaptive_transformer">Code</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/adaptive_transformer.bib">Bibtex</a>
        <a target="_blank" href="http://slideslive.com/38928637">Presentation Video</a>
    </span>
</p>
<img src="/img/adaptive/alpha.png">
<hr>

<a name="/detection"></a>
<h2 class="pubt">On Generalization of Detection Models for Unconstrained Environments</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1909.13080">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection">Code</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/gen_detection_models_iccvw19.bib">Bibtex</a>
        <a target="_blank" href="https://docs.google.com/presentation/d/1q6alY-5pRsJ2ys_402dhEOG0pVKk1VElDbegcXFFzoA/edit?usp=drivesdk">Poster</a>

    </span>
</p>
<img src="/img/detection/pipeline.png">
<hr>

<a name="/incremental"></a>
<h2 class="pubt">Incremental Learning in Person Re-Identification</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1909.13080">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection">Code</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/gen_detection_models_iccvw19.bib">Bibtex</a>
        <a target="_blank" href="https://docs.google.com/presentation/d/1q6alY-5pRsJ2ys_402dhEOG0pVKk1VElDbegcXFFzoA/edit?usp=drivesdk">Poster</a>

    </span>
</p>
<img src="/img/incremental/our_arch.png">
<hr>

<a name="/projects"></a>

# Side projects

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://github.com/prajjwal1/fluence">fluence</a></h2>
        <p class="talkd">
            Winner of Pytorch Global Hackathon 2020. A Pytorch deep learning library focussed on
            providing support for compute efficient and debiasing algorithms in transformer based
            model for NLP research. Contains implementation of Adaptive Attention, Sparsity, Layerdrop,
            Debiasing, Pruning utilities etc.
            <a target="_blank" href="http://github.com/prajjwal1/fluence"><img style="margin-top: 10px;" src="/img/projects/ai-paygrades.png"></a>
        </p>
    </div>
</div>


### Autonomous Object Detection

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/prajjwal1">Open source contributions</a></h2>
        <p class="talkd">
            I've made several contributions to [Pytorch](https://github.com/pytorch/pytorch/pulls?q=is%3Apr+author%3Aprajjwal1+is%3Aclosed) and [Huggingface Transformers](https://github.com/huggingface/transformers/pulls?q=is%3Apr+author%3Aprajjwal1+is%3Aclosed)
            <a target="_blank" href="https://github.com/prajjwal1"><img class="project-img" src="/img/projects/neural-vqa-attention.jpg"></a>
        </p>
    </div>
</div>


<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection">Autonomous Object Detection</a></h2>
        <p class="talkd">
            This [project](https://github.com/prajjwal1/autonomous-object-detection) focussed on 2D object detection with Pytorch. User can leverage models provided from `torchvision` and use datasets provided in this project (`idd`, `cityscapes`, `bdd`) for training and evaluation of models. Additionally, support for incremental learning was added.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa"><img src="/img/projects/neural-vqa.jpg"></a>
        </p>
    </div>
</div>

<!-- <div class="row"> -->
    <!-- <div class="col-sm-12"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://erdos.sdslabs.co">Erdős</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- Erdős by <a target="_blank" href="//sdslabs.co">SDSLabs</a> is a competitive math learning platform, similar in spirit to <a href="https://projecteuler.net/">Project Euler</a>, albeit more feature-packed (support for holding competitions, has a social layer) and prettier. -->
            <!-- <a target="_blank" href="https://erdos.sdslabs.co"><img style="margin-top:10px;" src="/img/projects/erdos.jpg"></a> -->
        <!-- </p> -->
    <!-- </div> -->
<!-- </div> -->

<!-- <div class="row"> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/graf">graf</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- graf plots pretty git contribution bar graphs in the terminal. -->
            <!-- <code>gem install graf</code> to install. -->
            <!-- <a target="_blank" href="https://github.com/abhshkdz/graf"><img style="margin-top:10px;" src="/img/projects/graf.gif"></a> -->
        <!-- </p> -->
    <!-- </div> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/HackFlowy">HackFlowy</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- Clone of <a href="//workflowy.com">WorkFlowy.com</a>, a beautiful, list-based note-taking website that has a 500-item monthly limit on the free tier :-(. This project is an open-source clone of WorkFlowy. "Make lists. Not war." :-) -->
            <!-- <a target="_blank" href="https://github.com/abhshkdz/HackFlowy"><img style="margin-top:40px;" src="/img/projects/hackflowy.png"></a> -->
        <!-- </p> -->
    <!-- </div> -->
<!-- </div> -->

<!-- <div class="row"> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/AirMaps">AirMaps</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- AirMaps was a fun hackathon project that lets users navigate through Google Earth with gestures and speech commands using a Kinect sensor. It was the <a target="_blank" href="https://blog.sdslabs.co/2014/02/code-fun-do">winning entry in Microsoft Code.Fun.Do</a>. -->
            <!-- <a target="_blank" href="https://github.com/abhshkdz/AirMaps"><img style="margin-top:10px;" src="/img/projects/airmaps.jpg"></a> -->
        <!-- </p> -->
    <!-- </div> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/sdslabs/hackview">HackView</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- Another fun hackathon-winning project built during Yahoo! HackU! 2012 that involves webRTC-based P2P video chat, and was faster than any other video chat provider (at the time, before Google launched Hangouts). -->
        <!-- </p> -->
    <!-- </div> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/8tracks-downloader">8tracks-downloader</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- Ugly-looking, but super-effective bash script for downloading entire playlists from 8tracks. (Still works as of 10/2016). -->
        <!-- </p> -->
    <!-- </div> -->
<!-- < -->/div>

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---

[1]: //mlp.cc.gatech.edu
[2]: ///www.cc.gatech.edu/~dbatra/
[3]: //www.cc.gatech.edu/~parikh/
[4]: //www.qbi.uq.edu.au/professor-geoffrey-goodhill
[5]: //researchers.uq.edu.au/researcher/2490
[6]: http://cns.qbi.uq.edu.au/
[7]: //developers.google.com/open-source/gsoc/
[8]: /posts/summer-of-code/
[9]: /posts/gsoc-reunion-2014/
[10]: //blog.sdslabs.co/2012/09/hacku
[11]: //blog.sdslabs.co/2014/02/code-fun-do
[12]: //www.facebook.com/SDSLabs/posts/527540147292475
[13]: /posts/deloitte-cctc-3/
[14]: /posts/google-india-community-summit/
[15]: //blog.sdslabs.co/2013/10/syntax-error-2013
[16]: //sdslabs.co/
[17]: //erdos.sdslabs.co/
[18]: //projecteuler.net/
[19]: //github.com/abhshkdz/neural-vqa
[20]: //github.com/abhshkdz/HackFlowy
[21]: //github.com/abhshkdz/graf
[22]: //github.com/abhshkdz
[23]: //twitter.com/abhshkdz
[24]: //instagram.com/abhshkdz
[25]: http://x.abhishekdas.com/
[26]: https://abhishekdas.com/vqa-hat/
[27]: http://arxiv.org/abs/1606.03556
[28]: https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/
[29]: https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/
[30]: http://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes
[31]: http://iitr.ac.in/
[32]: https://www.facebook.com/dhruv.batra.1253/posts/1783087161932290
[33]: https://drive.google.com/file/d/1nObeNzl-sTy8I5QN1Jv8wscebKLv-6RY/view?usp=sharing
[34]: http://aideadlin.es/
[35]: //github.com/abhshkdz/neural-vqa-attention
[36]: https://snapresearchfellowship.splashthat.com/
[37]: https://www.youtube.com/watch?v=R4hugGnNr7s
[38]: https://www.youtube.com/watch?v=I9OlorMh7wU
[39]: https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/
[40]: https://embodiedqa.org/
[41]: https://youtu.be/KAlGWMJnWyc?t=26m56s
[42]: https://2018gputechconf.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=152715
[43]: https://www.ic.gatech.edu/news/600684/three-ic-students-earn-snap-research-awards
[44]: https://www.ic.gatech.edu/news/601084/new-research-fellowships-offer-two-students-funding-access-adobes-creative-cloud
[45]: https://github.com/facebookresearch/House3D
[46]: https://gkioxari.github.io/
[47]: https://research.fb.com/people/parikh-devi/
[48]: https://research.fb.com/people/batra-dhruv/
[49]: https://lvatutorial.github.io/
[50]: http://acl2018.org/tutorials/#connecting-language-and-vis
[51]: http://visualqa.org/workshop.html
[52]: http://on-demand.gputechconf.com/gtc/2018/video/S8582/
[53]: https://visualdialog.org/challenge/2018
[54]: https://youtu.be/gz2VoDrvX-A?t=1h19m58s
[55]: https://research.fb.com/people/rabbat-mike/
[56]: https://www.cs.mcgill.ca/~jpineau/
[57]: https://visualdialog.org/challenge/2018#winners
[58]: https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330
[fb-fellow-page]: https://research.fb.com/announcing-the-2019-facebook-fellows-and-emerging-scholars/
[joelle-corl18-talk-mention]: https://www.youtube.com/watch?v=FSsEqEJKo8A&t=3497
[visdial-challenge-2]: https://visualdialog.org/challenge/2019
[ic-gt-article]: https://www.ic.gatech.edu/news/617061/see-and-say-abhishek-das-working-provide-crucial-communication-tools-intelligent-agents
[caliper]: https://caliper.ai
[felix-hill]: https://fh295.github.io
[laura-rimell]: http://www.rimell.cc/laura/
[stephen-clark]: https://sites.google.com/site/stephenclark609/
[andrej-karpathy]: https://karpathy.ai/
[vigil19]: https://vigilworkshop.github.io/2019
[tarmac-icml-talk]: https://www.facebook.com/icml.imls/videos/444326646299556/
[mastodon]: https://mastodon.social/web/accounts/1011404
[conquerearth]: https://conquer.earth/abhshkdz
[qa-probing-icml20-talk]: https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering
[vigil20]: https://vigilworkshop.github.io
[ocp]: https://opencatalystproject.org
[ocp-cnbc]: https://www.cnbc.com/2020/10/14/facebook-to-use-ai-in-bid-to-improve-renewable-energy-storage.html
[ocp-engadget]: https://engadget.com/facebook-deploys-its-ai-to-find-green-energy-storage-solutions-130041147.html
[ocp-fortune]: https://fortune.com/2020/10/14/facebook-ai-open-catalyst-dataset-chemistry-renewable-energy/
[ocp-venturebeat]: https://venturebeat.com/2020/10/14/facebook-and-carnegie-mellon-launch-project-to-discover-better-ways-to-store-renewable-energy/
[aipaygrad.es]: https://aipaygrad.es
[sigma-xi-thesis-award]: https://cpb-us-w2.wpmucdn.com/sites.gatech.edu/dist/0/283/files/2021/03/2021-Sigma-Xi-Research-Award-Winners.final_.pdf
[coc-dissertation-award]: https://sites.gatech.edu/gtcomputingawards2021/graduate-student-awards/
[thesis-pdf]: https://drive.google.com/file/u/2/d/1b2Gonazl1Os0eLPV9frkucEqSuRroEvD/view?usp=sharing
[aaai-dissertation-award]: https://aaai.org/Awards/dissertation-award.php
