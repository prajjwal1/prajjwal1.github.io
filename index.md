---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        AI Resident<br>
        Facebook AI Research<br>
        prajj at fb dot com
    </div>
</div>
<hr>

<a name="/news"></a>

# News

* [Apr 22] I'll be joining Meta AI Research (formerly FAIR) as an AI Resident at Menlo Park.
* [Oct 21] [Survey paper](https://arxiv.org/abs/2201.12438) has been accepted at AAAI 2022.
* [Sept 21]: Paper accepted at EMNLP Insights from Negative Results Workshop 2021.
* [Oct 20]: Volunteer for Neurips 2020
* [Oct 20]: Declared winner of Pytorch Global Hackathon 2020 in the responsible AI category
* [Aug 20]: I'll be joining UT Dallas as an MSCS student
* [June 20]: Will serve as a volunteer for [ACL](https://acl2020.org/), Seattle 2020
* [June 20]: Secured 18th spot at CVPR 2020 [VQA challenge](https://visualqa.org/roe)
* [June 20]: Will serve as a volunteer for [ICML](https://icml.cc/Conferences/2020), Vienna 2020
* [May 20]: My paper on Adaptive Transformers is available on [arxiv](https://arxiv.org/abs/2005.07486)
* [Apr 20]: 1 Paper accepted at ACL SRW 2020
* [Apr 20]: I'll be a volunteer for ICLR 2020
* [Feb 20]: I joined Siemens again as a research intern to work on Predictive Maintenance.
* [Nov 19]: I’ll be doing the Poster presentation at ICCV 2019, Seoul.
* [August 19]: 1 paper accepted at ICCV workshop 2019
* [Jan 19]: I joined Siemens as a research intern (Autonomous Navigation group)

<div id="read-more-button">
    <a nohref>Read more</a>
</div>

<hr>

<a name="/bio"></a>

# Bio

I am an AI Resident at [Meta AI Research](https://ai.facebook.com) (FAIR) in the [Conversational AI team](https://ai.facebook.com/research/conversational-ai/) working
under [Chinnadhurai Sankar](https://chinnadhurai.github.io).
Previously, I was a CS graduate student at the University of Texas Dallas where I worked on commonsense reasoning under
[Prof. Vincent Ng](http://www.hlt.utdallas.edu/~vince/). My thesis is about improving commonsense reasoning through adversarial
learning (coming soon). My research interests lie in the general area of deep learning and representation learning, especially at the intersection
of computer vision and language.

**Vision**
I have previously worked on numerous domains in computer vision during my internships which includes
[Person Re-Identification](https://prajjwal1.github.io/publications/IncrementalPersonReid),
[Depth Perception and 3D Object Detection](https://www.youtube.com/watch?v=vlDTgj3Kut8). Out of my interests,
I've worked extensively on visual recognition based tasks and
[generalization of detection models](https://prajjwal1.github.io/publications/GenDetectionIccvw19).
I've also written numerous [blog posts](https://prajjwal1.github.io/blog/) about it.

**Language**
To explore further interests, I've made a transition to NLP especially language understanding and how we can form
generalized representations which can be harnessed for subsequent downstream tasks. I care about the accessibility
and impact marked by my research. Specifically, I looked at
[Adaptive methods](https://prajjwal1.github.io/publications/adaptive_tfmr_acl_srw_2020) and how can they be used to
create efficient Multi-modal transformer-based architectures. Recently I won
[Pytorch Global Summer Hackathon](https://pytorch.org/blog/announcing-the-winners-of-the-2020-global-pytorch-summer-hackathon/)
for my project [Fluence](https://github.com/prajjwal1/fluence). It focuses on addressing the task of language
understanding responsibly (fairness and computational efficiency).

My recent projects have looked more closely at reasoning.
[Generalization in NLI ](https://arxiv.org/abs/2110.01518) project looked at how out-of-domain generalization in
pre-trained Language Models can be improved from biased data (MNLI -> HANS). My graduate research looked specifically
at commonsense reasoning within pre-trained Langauge Models. The [AAAI survey](https://arxiv.org/abs/2201.12438) provides
a comprehensive look at the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and
generation.


I created a [Youtube channel](https://youtube.com/c/aijournal) to disseminate technical content from research papers.
After graduate school, my interest has changed about creating content as a result of which this project is
unmaintained right now.

<!-- <div class="row" id="timeline-logos"> -->
    <!-- <div class="col-xs-3"> -->
        <!-- <div class="logo-wrap"> -->
            <!-- <span class="helper"></span> -->
            <!-- <a href="//iitr.ac.in"><img src="/img/logos/iitr.jpg"></a> -->
        <!-- </div> -->
        <!-- <div class="logo-desc"> -->
            <!-- IIT Roorkee<br> -->
            <!-- 2011 - 2015 -->
        <!-- </div> -->
    <!-- </div> -->
    <!-- <div class="col-xs-3"> -->
        <!-- <div class="logo-wrap"> -->
            <!-- <span class="helper"></span> -->
            <!-- <a href="//qbi.uq.edu.au"><img style="width:120px;" src="/img/logos/uq.png"></a> -->
        <!-- </div> -->
        <!-- <div class="logo-desc"> -->
            <!-- Queensland Brain Institute<br> -->
            <!-- Summer 2015 -->
        <!-- </div> -->
    <!-- </div> -->
    <!-- <div class="col-xs-3"> -->
        <!-- <div class="logo-wrap"> -->
            <!-- <span class="helper"></span> -->
            <!-- <a href="//vt.edu"><img src="/img/logos/vt.png"></a> -->
        <!-- </div> -->
        <!-- <div class="logo-desc"> -->
            <!-- Virginia Tech<br> -->
            <!-- 2015 - 2016 -->
        <!-- </div> -->
    <!-- </div> -->
    <!-- <div class="col-xs-2"> -->
        <!-- <div class="logo-wrap"> -->
            <!-- <span class="helper"></span> -->
            <!-- <a target="_blank" href="//gatech.edu"><img src="/img/logos/gatech.png"></a> -->
        <!-- </div> -->
        <!-- <div class="logo-desc"> -->
            <!-- Georgia Tech<br> -->
            <!-- 2017 - 2020 -->
        <!-- </div> -->
    <!-- </div> -->
    <!-- <div class="col-xs-3"> -->
        <!-- <div class="logo-wrap"> -->
            <!-- <span class="helper"></span> -->
            <!-- <a target="_blank" href="//research.fb.com/category/facebook-ai-research/"><img style="width:160px;" src="/img/logos/fair3.png"></a> -->
        <!-- </div> -->
        <!-- <div class="logo-desc"> -->
            <!-- Facebook AI Research<br> -->
            <!-- S2017, W2018, S2018 -->
        <!-- </div> -->
    <!-- </div> -->
    <!-- <div class="col-xs-3"> -->
        <!-- <div class="logo-wrap"> -->
            <!-- <span class="helper"></span> -->
            <!-- <a target="_blank" href="//deepmind.com"><img style="width:120px;" src="/img/logos/deepmind.png"></a> -->
        <!-- </div> -->
        <!-- <div class="logo-desc"> -->
            <!-- DeepMind<br> -->
            <!-- Winter 2019 -->
        <!-- </div> -->
    <!-- </div> -->
    <!-- <div class="col-xs-3"> -->
        <!-- <div class="logo-wrap"> -->
            <!-- <span class="helper"></span> -->
            <!-- <a target="_blank" href="//www.tesla.com/autopilotAI"><img style="width:120px;" src="/img/logos/tesla.jpg"></a> -->
        <!-- </div> -->
        <!-- <div class="logo-desc"> -->
            <!-- Tesla Autopilot<br> -->
            <!-- Summer 2019 -->
        <!-- </div> -->
    <!-- </div> -->
    <!-- <div class="col-xs-3"> -->
        <!-- <div class="logo-wrap"> -->
            <!-- <span class="helper"></span> -->
            <!-- <a target="_blank" href="//research.fb.com/category/facebook-ai-research/"><img style="width:160px;" src="/img/logos/fair3.png"></a> -->
        <!-- </div> -->
        <!-- <div class="logo-desc"> -->
            <!-- Facebook AI Research<br> -->
            <!-- Present -->
        <!-- </div> -->
    <!-- </div> -->
<!-- </div> -->

<!-- During my PhD, I interned thrice at Facebook AI Research — Summer 2017 and Spring 2018 -->
<!-- at Menlo Park, working with [Georgia Gkioxari][46], -->
<!-- [Devi Parikh][47] and [Dhruv Batra][48] on training embodied agents for navigation and -->
<!-- question-answering in simulated environments (see [embodiedqa.org][40]), and Summer -->
<!-- 2018 at Montréal, working with [Mike Rabbat][55] and [Joelle Pineau][56] on -->
<!-- <a target="_blank" href="https://arxiv.org/abs/1810.11187">communication protocols in multi-agent reinforcement learning</a>. -->
<!-- In 2019, I interned at DeepMind in London working on grounded language learning -->
<!-- with [Felix Hill][felix-hill], [Laura Rimell][laura-rimell], -->
<!-- and [Stephen Clark][stephen-clark], and at Tesla Autopilot in Palo Alto working on -->
<!-- differentiable neural architecture search with [Andrej Karpathy][andrej-karpathy]. -->

<!-- My PhD research was supported by fellowships from [Facebook][fb-fellow-page], -->
<!-- [Adobe][39], and [Snap][36]. -->

<!-- Prior to joining grad school, I worked on neural coding in zebrafish tectum -->
<!-- as an intern under [Prof. Geoffrey Goodhill][4] and [Lilach Avitan][5] -->
<!-- at the [Goodhill Lab][6], Queensland Brain Institute. -->

<!-- I got my Bachelor's at [IIT Roorkee][31] in 2015. -->
<!-- During my undergrad, I took part in -->
<!-- Google Summer of Code ([2013][8] and [2014][9]), -->
<!-- won several competitions ([Yahoo! HackU!][10], -->
<!-- [Microsoft Code.Fun.Do.][11], Deloitte CCTC [2013][12] and [2014][13]), -->
<!-- and owe most of my programming/tinkering bent to [SDSLabs][16]. -->

<!-- On the side, I built [aideadlin.es][34] (countdowns to a bunch of CV/NLP/ML/AI conference deadlines) -->
<!-- and [aipaygrad.es][aipaygrad.es] (statistics of industry job offers in AI), -->
<!-- [neural-vqa][19] and its extension [neural-vqa-attention][35], -->
<!-- [HackFlowy][20], [graf][21], [Erdős][17], [etc][22]. -->
<!-- I also occasionally dabble in [generative art](/art). -->
<!-- I like [this map][conquerearth] tracking the places I've been to. -->
<!-- [Blog posts from a previous life](/archive). -->

---

<a name="/publications"></a>

# Publications

<a name="/gemnet-oc"></a>
<h2 class="pubt">Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava and Vincent Ng</span><br>
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2201.12438">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/aaai-22/survey_poster.pdf">Poster</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/commonsense_survey_aaai_22.bib">Bibtex</a>
    </span>
</p>
<img src="/img/ocp/gemnet_oc.jpg">
<hr>

<a name="/habitat-web"></a>
<h2 class="pubt">Generalization in NLI: Ways to [Not] Go Beyond Simple Heuristics</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava, Aleksander Drozd, Anna Rogers</span><br>
    <span class="conf">CVPR 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2110.01518">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/generalize_lm_nli">Code (Huggingface)</a>
        <a target="_blank" href="https://github.com/vecto-ai/langmo">Code (Pytorch Lightning)</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/generalize_lm_nli.bib">Bibtex</a>
        <a target="_blank" href="https://www.youtube.com/watch?v=ByQu3J6Ji7E">Presentation video</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/emnlp-21/NLI_Generalization_EMNLP_Poster.pdf">Poster</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/emnlp-21/chinese_room_presentation.pdf">Slides</a>

    </span>
</p>
<img src="/img/habitat/habitat-web.gif">
<hr>

<a name="/graph-parallel"></a>
<h2 class="pubt">Adaptive Transformers for Learning Multimodal Representations</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava</span><br>
    <span class="conf">ICLR 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2005.07486">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/adaptive_transformer">Code</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/adaptive_transformer.bib">Bibtex</a>
        <a target="_blank" href="http://slideslive.com/38928637">Presentation Video</a>
    </span>
</p>
<img src="/img/ocp/graph_parallel.png">
<hr>

<a name="/spinconv"></a>
<h2 class="pubt">On Generalization of Detection Models for Unconstrained Environments</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava</span><br>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1909.13080">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection">Code</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/gen_detection_models_iccvw19.bib">Bibtex</a>
        <a target="_blank" href="https://docs.google.com/presentation/d/1q6alY-5pRsJ2ys_402dhEOG0pVKk1VElDbegcXFFzoA/edit?usp=drivesdk">Poster</a>

    </span>
</p>
<img src="/img/ocp/spinconv.jpg">
<hr>


<!-- <a name="/talks"></a> -->

<!-- # Talks -->

<!-- <div class="row"> -->
    <!-- <div class="col-xs-6"> -->
        <!-- <p class="talkd"> -->
            <!-- <img src="/img/talks/visdial_rl_iccv17.jpg"> -->
        <!-- </p> -->
    <!-- </div> -->
    <!-- <div class="col-xs-6"> -->
        <!-- <p class="talkd"> -->
            <!-- <img src="/img/talks/embodiedqa_cvpr18_4.jpg"> -->
        <!-- </p> -->
    <!-- </div> -->
<!-- </div> -->
<!-- <div class="row"> -->
    <!-- <div class="col-xs-12"> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering"> -->
                <!-- ICML 2020: Probing Emergent Semantics in Predictive Agents via Question Answering -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://slideslive.com/38917625/tarmac-targeted-multiagent-communication"> -->
                <!-- ICML 2019 Imitation, Intent, and Interaction Workshop: -->
                <!-- Targeted Multi-Agent Communication -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://www.facebook.com/icml.imls/videos/444326646299556/"> -->
                <!-- ICML 2019 Oral: Targeted Multi-Agent Communication -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://www.youtube.com/watch?v=WxYBp3Xr_Nc"> -->
                <!-- Allen Institute for Artificial Intelligence: "Towards Agents that can See, Talk, and Act" -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330"> -->
                <!-- CoRL 2018 Spotlight: Neural Modular Control for Embodied Question Answering -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://youtu.be/gz2VoDrvX-A?t=1h19m58s"> -->
                <!-- CVPR 2018 Oral: Embodied Question Answering -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="http://on-demand.gputechconf.com/gtc/2018/video/S8582/"> -->
                <!-- NVIDIA GTC 2018 -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://www.youtube.com/watch?v=R4hugGnNr7s"> -->
                <!-- ICCV 2017 Oral: Learning Cooperative Visual Dialog Agents with Deep RL -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://youtu.be/KAlGWMJnWyc?t=26m56s"> -->
                <!-- Visual Question Answering Challenge Workshop, CVPR 2017 -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="https://www.youtube.com/watch?v=I9OlorMh7wU"> -->
                <!-- CVPR 2017 Spotlight: Visual Dialog -->
            <!-- </a> -->
        <!-- </div> -->
        <!-- <div class="talkt"> -->
            <!-- <a target="_blank" href="http://techtalks.tv/talks/towards-transparent-visual-question-answering-systems/63026/"> -->
                <!-- Visualization for Deep Learning Workshop, ICML 2016 -->
            <!-- </a> -->
        <!-- </div> -->
    <!-- </div> -->
<!-- </div> -->
<!-- <hr> -->

<a name="/projects"></a>

# Side projects

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://github.com/prajjwal1/fluence">fluence</a></h2>
        <p class="talkd">
            Winner of Pytorch Global Hackathon 2020. A Pytorch deep learning library focussed on
            providing support for compute efficient and debiasing algorithms in transformer based
            model for NLP research. Contains implementation of Adaptive Attention, Sparsity, Layerdrop,
            Debiasing, Pruning utilities etc.
            <a target="_blank" href="http://github.com/prajjwal1/fluence"><img style="margin-top: 10px;" src="/img/projects/ai-paygrades.png"></a>
        </p>
    </div>
</div>


### Autonomous Object Detection

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/prajjwal1">Open source contributions</a></h2>
        <p class="talkd">
            I've made several contributions to [Pytorch](https://github.com/pytorch/pytorch/pulls?q=is%3Apr+author%3Aprajjwal1+is%3Aclosed) and [Huggingface Transformers](https://github.com/huggingface/transformers/pulls?q=is%3Apr+author%3Aprajjwal1+is%3Aclosed)
            <a target="_blank" href="https://github.com/prajjwal1"><img class="project-img" src="/img/projects/neural-vqa-attention.jpg"></a>
        </p>
    </div>
</div>


<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection">Autonomous Object Detection</a></h2>
        <p class="talkd">
            This [project](https://github.com/prajjwal1/autonomous-object-detection) focussed on 2D object detection with Pytorch. User can leverage models provided from `torchvision` and use datasets provided in this project (`idd`, `cityscapes`, `bdd`) for training and evaluation of models. Additionally, support for incremental learning was added.
            <a target="_blank" href="https://github.com/abhshkdz/neural-vqa"><img src="/img/projects/neural-vqa.jpg"></a>
        </p>
    </div>
</div>

<!-- <div class="row"> -->
    <!-- <div class="col-sm-12"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://erdos.sdslabs.co">Erdős</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- Erdős by <a target="_blank" href="//sdslabs.co">SDSLabs</a> is a competitive math learning platform, similar in spirit to <a href="https://projecteuler.net/">Project Euler</a>, albeit more feature-packed (support for holding competitions, has a social layer) and prettier. -->
            <!-- <a target="_blank" href="https://erdos.sdslabs.co"><img style="margin-top:10px;" src="/img/projects/erdos.jpg"></a> -->
        <!-- </p> -->
    <!-- </div> -->
<!-- </div> -->

<!-- <div class="row"> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/graf">graf</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- graf plots pretty git contribution bar graphs in the terminal. -->
            <!-- <code>gem install graf</code> to install. -->
            <!-- <a target="_blank" href="https://github.com/abhshkdz/graf"><img style="margin-top:10px;" src="/img/projects/graf.gif"></a> -->
        <!-- </p> -->
    <!-- </div> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/HackFlowy">HackFlowy</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- Clone of <a href="//workflowy.com">WorkFlowy.com</a>, a beautiful, list-based note-taking website that has a 500-item monthly limit on the free tier :-(. This project is an open-source clone of WorkFlowy. "Make lists. Not war." :-) -->
            <!-- <a target="_blank" href="https://github.com/abhshkdz/HackFlowy"><img style="margin-top:40px;" src="/img/projects/hackflowy.png"></a> -->
        <!-- </p> -->
    <!-- </div> -->
<!-- </div> -->

<!-- <div class="row"> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/AirMaps">AirMaps</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- AirMaps was a fun hackathon project that lets users navigate through Google Earth with gestures and speech commands using a Kinect sensor. It was the <a target="_blank" href="https://blog.sdslabs.co/2014/02/code-fun-do">winning entry in Microsoft Code.Fun.Do</a>. -->
            <!-- <a target="_blank" href="https://github.com/abhshkdz/AirMaps"><img style="margin-top:10px;" src="/img/projects/airmaps.jpg"></a> -->
        <!-- </p> -->
    <!-- </div> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/sdslabs/hackview">HackView</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- Another fun hackathon-winning project built during Yahoo! HackU! 2012 that involves webRTC-based P2P video chat, and was faster than any other video chat provider (at the time, before Google launched Hangouts). -->
        <!-- </p> -->
    <!-- </div> -->
    <!-- <div class="col-sm-6"> -->
        <!-- <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/abhshkdz/8tracks-downloader">8tracks-downloader</a></h2> -->
        <!-- <p class="talkd"> -->
            <!-- Ugly-looking, but super-effective bash script for downloading entire playlists from 8tracks. (Still works as of 10/2016). -->
        <!-- </p> -->
    <!-- </div> -->
<!-- < -->/div>

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---

[1]: //mlp.cc.gatech.edu
[2]: ///www.cc.gatech.edu/~dbatra/
[3]: //www.cc.gatech.edu/~parikh/
[4]: //www.qbi.uq.edu.au/professor-geoffrey-goodhill
[5]: //researchers.uq.edu.au/researcher/2490
[6]: http://cns.qbi.uq.edu.au/
[7]: //developers.google.com/open-source/gsoc/
[8]: /posts/summer-of-code/
[9]: /posts/gsoc-reunion-2014/
[10]: //blog.sdslabs.co/2012/09/hacku
[11]: //blog.sdslabs.co/2014/02/code-fun-do
[12]: //www.facebook.com/SDSLabs/posts/527540147292475
[13]: /posts/deloitte-cctc-3/
[14]: /posts/google-india-community-summit/
[15]: //blog.sdslabs.co/2013/10/syntax-error-2013
[16]: //sdslabs.co/
[17]: //erdos.sdslabs.co/
[18]: //projecteuler.net/
[19]: //github.com/abhshkdz/neural-vqa
[20]: //github.com/abhshkdz/HackFlowy
[21]: //github.com/abhshkdz/graf
[22]: //github.com/abhshkdz
[23]: //twitter.com/abhshkdz
[24]: //instagram.com/abhshkdz
[25]: http://x.abhishekdas.com/
[26]: https://abhishekdas.com/vqa-hat/
[27]: http://arxiv.org/abs/1606.03556
[28]: https://www.newscientist.com/article/2095616-robot-eyes-and-humans-fix-on-different-things-to-decode-a-scene/
[29]: https://www.technologyreview.com/s/601819/ai-is-learning-to-see-the-world-but-not-the-way-humans-do/
[30]: http://www.theverge.com/2016/7/12/12158238/first-click-deep-learning-algorithmic-black-boxes
[31]: http://iitr.ac.in/
[32]: https://www.facebook.com/dhruv.batra.1253/posts/1783087161932290
[33]: https://drive.google.com/file/d/1nObeNzl-sTy8I5QN1Jv8wscebKLv-6RY/view?usp=sharing
[34]: http://aideadlin.es/
[35]: //github.com/abhshkdz/neural-vqa-attention
[36]: https://snapresearchfellowship.splashthat.com/
[37]: https://www.youtube.com/watch?v=R4hugGnNr7s
[38]: https://www.youtube.com/watch?v=I9OlorMh7wU
[39]: https://adoberesearch.ctlprojects.com/fellowship/previous-fellowship-award-winners/
[40]: https://embodiedqa.org/
[41]: https://youtu.be/KAlGWMJnWyc?t=26m56s
[42]: https://2018gputechconf.smarteventscloud.com/connect/sessionDetail.ww?SESSION_ID=152715
[43]: https://www.ic.gatech.edu/news/600684/three-ic-students-earn-snap-research-awards
[44]: https://www.ic.gatech.edu/news/601084/new-research-fellowships-offer-two-students-funding-access-adobes-creative-cloud
[45]: https://github.com/facebookresearch/House3D
[46]: https://gkioxari.github.io/
[47]: https://research.fb.com/people/parikh-devi/
[48]: https://research.fb.com/people/batra-dhruv/
[49]: https://lvatutorial.github.io/
[50]: http://acl2018.org/tutorials/#connecting-language-and-vis
[51]: http://visualqa.org/workshop.html
[52]: http://on-demand.gputechconf.com/gtc/2018/video/S8582/
[53]: https://visualdialog.org/challenge/2018
[54]: https://youtu.be/gz2VoDrvX-A?t=1h19m58s
[55]: https://research.fb.com/people/rabbat-mike/
[56]: https://www.cs.mcgill.ca/~jpineau/
[57]: https://visualdialog.org/challenge/2018#winners
[58]: https://www.youtube.com/watch?v=xoHvho-YRgs&t=7330
[fb-fellow-page]: https://research.fb.com/announcing-the-2019-facebook-fellows-and-emerging-scholars/
[joelle-corl18-talk-mention]: https://www.youtube.com/watch?v=FSsEqEJKo8A&t=3497
[visdial-challenge-2]: https://visualdialog.org/challenge/2019
[ic-gt-article]: https://www.ic.gatech.edu/news/617061/see-and-say-abhishek-das-working-provide-crucial-communication-tools-intelligent-agents
[caliper]: https://caliper.ai
[felix-hill]: https://fh295.github.io
[laura-rimell]: http://www.rimell.cc/laura/
[stephen-clark]: https://sites.google.com/site/stephenclark609/
[andrej-karpathy]: https://karpathy.ai/
[vigil19]: https://vigilworkshop.github.io/2019
[tarmac-icml-talk]: https://www.facebook.com/icml.imls/videos/444326646299556/
[mastodon]: https://mastodon.social/web/accounts/1011404
[conquerearth]: https://conquer.earth/abhshkdz
[qa-probing-icml20-talk]: https://slideslive.com/38928261/probing-emergent-semantics-in-predictive-agents-via-question-answering
[vigil20]: https://vigilworkshop.github.io
[ocp]: https://opencatalystproject.org
[ocp-cnbc]: https://www.cnbc.com/2020/10/14/facebook-to-use-ai-in-bid-to-improve-renewable-energy-storage.html
[ocp-engadget]: https://engadget.com/facebook-deploys-its-ai-to-find-green-energy-storage-solutions-130041147.html
[ocp-fortune]: https://fortune.com/2020/10/14/facebook-ai-open-catalyst-dataset-chemistry-renewable-energy/
[ocp-venturebeat]: https://venturebeat.com/2020/10/14/facebook-and-carnegie-mellon-launch-project-to-discover-better-ways-to-store-renewable-energy/
[aipaygrad.es]: https://aipaygrad.es
[sigma-xi-thesis-award]: https://cpb-us-w2.wpmucdn.com/sites.gatech.edu/dist/0/283/files/2021/03/2021-Sigma-Xi-Research-Award-Winners.final_.pdf
[coc-dissertation-award]: https://sites.gatech.edu/gtcomputingawards2021/graduate-student-awards/
[thesis-pdf]: https://drive.google.com/file/u/2/d/1b2Gonazl1Os0eLPV9frkucEqSuRroEvD/view?usp=sharing
[aaai-dissertation-award]: https://aaai.org/Awards/dissertation-award.php
