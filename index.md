---
layout:     page
title:
permalink:  /
---

<div class="row">
    <div class="col-sm-6 col-xs-12">
        <img src="/img/cover2.jpg">
    </div>
    <div class="col-sm-6 col-xs-12" style="margin-bottom: 0;">
        Research Engineer<br>
        Meta Superintelligence Labs<br>
        prajj at meta dot com
    </div>
</div>
<hr>

<a name="/bio"></a>

# Bio

I'm Praj, I work as an AI Researcher at [Meta Superintelligence Labs](https://ai.facebook.com) working on building foundational models(https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), next generation of LLaMA models. I am a core contributor of [LLaMA 4](https://llama.meta.com), [LLaMA 3](https://llama.meta.com) [LLaMA 2](https://ai.meta.com/llama/), [LLaMA 2 Long](https://arxiv.org/abs/2309.16039), powering Meta's flagship AI assistant [meta.ai](https://meta.ai). My primary focus for LLaMA 4 was improving long context capabilities (modeling and pre-training infra). LLaMA 4 can attend to 10M tokens long documents. Previously I worked as an AI Resident within [Reality Labs](https://about.meta.com/realitylabs/) and [Fundamental AI Research (FAIR)](https://ai.meta.com/research/) at Meta working on Offline Reinforcement Learning. My google scholar can be found [here](https://scholar.google.com/citations?user=zTq103EAAAAJ&hl).

Prior to Meta, I was a CS graduate student at the University of Texas Dallas where I worked on commonsense reasoning under the supervision of [Prof. Vincent Ng](http://www.hlt.utdallas.edu/~vince/). My [thesis](https://libtreasures.utdallas.edu/handle/10735.1/9511) is about improving commonsense reasoning through adversarial learning. 

<a name="/publications"></a>

# Publications
<img src="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/img/llama_4.png">
<hr>
<h2 class="pubt">The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</h2>
<p class="pubd">
    <span class="authors">Generative AI, Meta
    </span><br>
<!--     <span class="conf">arXiv 2023</span> -->
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">Paper</a>
</span>
</p>

<img src="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/img/llama_3.png">
<hr>
<h2 class="pubt">The LLaMA 3 herd of models</h2>
<p class="pubd">
    <span class="authors">Generative AI, Meta
    </span><br>
<!--     <span class="conf">arXiv 2023</span> -->
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2407.21783">Paper</a>
</span>
</p>

<img src="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/img/scaling_llama_cl.png">
<hr>
<h2 class="pubt">Effective Long-Context Scaling of Foundation Models</h2>
<p class="pubd">
    <span class="authors">W. Xiong, J. Liu, I. Molybog, H. Zhang, <b> P. Bhargava </b>, R. Hou, L. Martin, R. Rungta, K. Sankararaman, B. Oguz, M. Khabsa, H. Fang, Y. Mehdad, S. Narang, K. Malik, A. Fan, S. Bhosale, S. Edunov, M. Lewis, S. Wang, H. Ma
    </span><br>
<!--     <span class="conf">arXiv 2023</span> -->
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2309.16039">Paper</a>
</span>
</p>



<a name="/llama-v2"></a>


<img src="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/llama.png">
<hr>
<h2 class="pubt">Llama 2: Open Foundation and Fine-Tuned Chat Models</h2>
<p class="pubd">
    <span class="authors">H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, <b> P. Bhargava </b>, S. Bhosale, D. Bikel, L. Blecher, C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie,
A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. Smith, R. Subramanian,
X. Tan, B. Tang, R. Taylor, A. Williams, J. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan,
M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, T. Scialom
    </span><br>
<!--     <span class="conf">arXiv 2023</span> -->
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2307.09288">Paper</a>

<a target="_blank" href="https://ai.meta.com/llama/">Official Announcement</a>
<a target="_blank" href="https://github.com/facebookresearch/llama">Code</a>

<!--         <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/img/seq_model_offline_rl/seq_model_offline_rl.png">Poster</a> -->

<!-- <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/seq_model_offline_rl.bib">Bibtex</a> -->
</span>
</p>
<img src="/img/seq_model_offline_rl/seq_model_offline_rl.png">
<hr>

<a name="/seq-model-offline-rl"></a>
<h2 class="pubt">Sequence Modeling is a Robust Contender for Offline Reinforcement Learning</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani, Amy Zhang </span><br>
        <span class="conf">International Conference on Learning Representations (ICLR) 2024 </span>
<!--     <span class="conf">arXiv 2023</span> -->
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2305.14550">arXiv</a>
<!--         <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/img/seq_model_offline_rl/seq_model_offline_rl.png">Poster</a> -->
        <a target="_blank" href="https://github.com/prajjwal1/rl_paradigms">Code</a>
<a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/seq_model_offline_rl.bib">Bibtex</a>
    </span>
</p>
<img src="/img/seq_model_offline_rl/seq_model_offline_rl.png">
<hr>

<a name="/autodial"></a>
<h2 class="pubt">AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava, Pooyan Amini, Shahin Shayandeh, Chinnadhurai Sankar </span><br>
<!--     <span class="conf">EMNLP 2022</span> -->
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2303.06245">arXiv</a>
<!--         <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/img/emnlp_22/af.png">Poster</a> -->
        <a target="_blank" href="https://github.com/prajjwal1/autodial">Code</a>
<a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/autodial.bib">Bibtex</a>
    </span>
</p>
<img src="/img/autodial/autodial.png">
<hr>




<a name="/discosense"></a>
<h2 class="pubt">DiscoSense: Commonsense Reasoning with Discourse Relations</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava and Vincent Ng</span><br>
    <span class="conf">EMNLP 2022</span>
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2210.12478">arXiv</a>
<!--         <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/img/emnlp_22/af.png">Poster</a> -->
        <a target="_blank" href="https://github.com/prajjwal1/discosense">Code</a>
<a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/discosense.bib">Bibtex</a>
    </span>
</p>
<img src="/img/emnlp_22/af.png">
<hr>

<a name="/commonsense-survey"></a>
<h2 class="pubt">Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava and Vincent Ng</span><br>
    <span class="conf">AAAI 2022</span>
    <span class="conf"></span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2201.12438">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/aaai-22/survey_poster.pdf">Poster</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/commonsense_survey_aaai_22.bib">Bibtex</a>
    </span>
</p>
<img src="/img/survey/probe.png">
<hr>

<a name="/generalization-nli"></a>
<h2 class="pubt">Generalization in NLI: Ways to [Not] Go Beyond Simple Heuristics</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava, Aleksander Drozd, Anna Rogers</span><br>
    <span class="conf">EMNLP Workshop on Insights from Negative Results 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2110.01518">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/generalize_lm_nli">Code (Huggingface)</a>
        <a target="_blank" href="https://github.com/vecto-ai/langmo">Code (Pytorch Lightning)</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/generalize_lm_nli.bib">Bibtex</a>
        <a target="_blank" href="https://aclanthology.org/2021.insights-1.18.mp4">Presentation video</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/emnlp-21/NLI_Generalization_EMNLP_Poster.pdf">Poster</a>
        <a target="_blank" href="https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/emnlp-21/chinese_room_presentation.pdf">Slides</a>

    </span>
</p>
<img src="/img/generalization_nli/MNLI_roberta-large.png">
<hr>

<a name="/adaptive"></a>
<h2 class="pubt">Adaptive Transformers for Learning Multimodal Representations</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava</span><br>
    <span class="conf">ACL SRW 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/2005.07486">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/adaptive_transformer">Code</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/adaptive_transformer.bib">Bibtex</a>
        <a target="_blank" href="http://slideslive.com/38928637">Presentation Video</a>
    </span>
</p>
<img src="/img/adaptive/alpha.png">
<hr>

<a name="/detection"></a>
<h2 class="pubt">On Generalization of Detection Models for Unconstrained Environments</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava</span><br>
    <span class="conf">ICCV AutoNUE Workshop 2022</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1909.13080">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection">Code</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/gen_detection_models_iccvw19.bib">Bibtex</a>
        <a target="_blank" href="https://docs.google.com/presentation/d/1q6alY-5pRsJ2ys_402dhEOG0pVKk1VElDbegcXFFzoA/edit?usp=drivesdk">Poster</a>
    </span>
</p>
<img src="/img/detection/pipeline.png">
<hr>

<a name="/incremental"></a>
<h2 class="pubt">Incremental Learning in Person Re-Identification</h2>
<p class="pubd">
    <span class="authors">Prajjwal Bhargava</span><br>
    <span class="conf">arXiv preprint</span>
    <span class="links">
        <a target="_blank" href="https://arxiv.org/abs/1909.13080">Paper</a>
        <a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection">Code</a>
        <a target="_blank" href="https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/gen_detection_models_iccvw19.bib">Bibtex</a>
        <a target="_blank" href="https://docs.google.com/presentation/d/1q6alY-5pRsJ2ys_402dhEOG0pVKk1VElDbegcXFFzoA/edit?usp=drivesdk">Poster</a>
    </span>
</p>
<img src="/img/incremental/our_arch.png">
<hr>

<a name="/projects"></a>

# Side projects

<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="http://github.com/prajjwal1/fluence">fluence</a></h2>
        <p class="talkd">
            Winner of Pytorch Global Hackathon 2020. A Pytorch deep learning library focussed on
            providing support for compute efficient and debiasing algorithms in transformer based
            model for NLP research. Contains implementation of Adaptive Attention, Sparsity, Layerdrop,
            Debiasing, Pruning utilities etc.
            <a target="_blank" href="http://github.com/prajjwal1/fluence"><img style="margin-top: 10px;" src="/img/projects/fluence.png"></a>
        </p>
    </div>
</div>


<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="/open-source">Open source contributions</a></h2>
        <p class="talkd">
                        Contributions to Pytorch Ecosystem
            <a target="_blank" href="/open-source"><img class="project-img" src="/img/projects/hf_pytorch.png"></a>
        </p>
    </div>
</div>


<div class="row">
    <div class="col-sm-12">
        <h2 class="talkt" style="font-weight:300;"><a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection">Autonomous Object Detection</a></h2>
        <p class="talkd">
            This project focussed on 2D object detection with Pytorch.
            User can leverage models provided from `torchvision` and use datasets provided in this project (`idd`, `cityscapes`, `bdd`)
            for training and evaluation of models. Additionally, support for incremental learning was added.
            <a target="_blank" href="https://github.com/prajjwal1/autonomous-object-detection"><img src="/img/projects/baseline_preds.png"></a>
        </p>
    </div>
</div>

<script src="/js/jquery.min.js"></script>
<script type="text/javascript">
    $('ul:gt(0) li:gt(12)').hide();
    $('#read-more-button > a').click(function() {
        $('ul:gt(0) li:gt(12)').show();
        $('#read-more-button').hide();
    });
</script>

---
