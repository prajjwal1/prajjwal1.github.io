---
title: "Adaptive Transformers for Learning Multimodal Representations"
collection: publications
permalink: /publications/adaptive_tfmr_acl_srw_2020
date: 17-04-2020
venue: 'ACL SRW'
author_profile: true
---

### Abstract
The usage of transformers has grown from learning about language semantics to forming meaningful
visiolinguistic representations. These architectures are often over-parametrized, requiring large
amounts of computation. In this work, we extend adaptive approaches to learn more about model
interpretability and computational efficiency. Specifically, we study attention spans, sparse, and
structured dropout methods to help understand how their attention mechanism extends for vision and
language tasks. We further show that these approaches can help us learn more about how the network
perceives the complexity of input sequences, sparsity preferences for different modalities, and other
related phenomena.

- [arXiv](https://arxiv.org/abs/2005.07486)
- [ACL Anthology](https://aclanthology.org/2020.acl-srw.1/)
- [Github](https://github.com/prajjwal1/adaptive_transformers)
- [bibtex](https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/adaptive_transformer.bib)
- [Video](http://slideslive.com/38928637)
- [Fluence (library)](https://github.com/prajjwal1/fluence)
