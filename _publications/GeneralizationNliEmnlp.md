---
title: "Generalization in NLI: Ways to [Not] Go Beyond Simple Heuristics"
collection: publications
permalink: /publications/GeneralizationNliEmnlp
date: 29-09-2021
venue: 'Workshop on Insights from Negative Results, EMNLP'
author_profile: true
#paperurl: ''
---
Abstract:
Much of recent progress in NLU was shown to be due to models' learning dataset-specific heuristics.
We conduct a case study of generalization in NLI (from MNLI to the adversarially constructed HANS
dataset) in a range of BERT-based architectures (adapters, Siamese Transformers, HEX debiasing),
as well as with subsampling the data and increasing the model size. We report 2 successful and 3
unsuccessful strategies, all providing insights into how Transformer-based models learn to generalize.

- [arXiv](https://arxiv.org/abs/2110.01518)
- [ACL Anthology](https://aclanthology.org/2021.insights-1.18/)
- [Code - Huggingface](https://github.com/prajjwal1/generalize_lm_nli)
- [Code - Pytorch Lightning](https://github.com/vecto-ai/langmo)
- [bibtex](https://raw.githubusercontent.com/prajjwal1/prajjwal1.github.io/master/bibtex/generalize_lm_nli.bib)
- [Video](https://www.youtube.com/watch?v=ByQu3J6Ji7E)
- [Slides](https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/emnlp-2021/generalization-nli/chinese_room_presentation.pdf)
- [Poster](https://github.com/prajjwal1/prajjwal1.github.io/raw/master/research/emnlp-2021/generalization-nli/NLI_Generalization_EMNLP_Poster.pdf)
